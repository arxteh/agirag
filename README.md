# Развертывание и использование Agi RAG

Этот документ содержит инструкции по развертыванию и использованию on-premise платформы для работы с документацией в режиме чата. Платформа объединяет в себе инструменты для обработки документов, векторного поиска, управления рабочими процессами и пользовательского интерфейса.

## Архитектура

Система состоит из следующих компонентов:

- **Open WebUI**: Пользовательский интерфейс для общения с чат-ботом.
- **n8n**: Инструмент автоматизации рабочих процессов.
- **Flowise**: Визуальный конструктор для создания LLM-приложений.
- **Docling**: Сервис для конвертации документов в машиночитаемый формат (Markdown).
- **LightRAG**: Система RAG (Retrieval-Augmented Generation) для поиска и генерации ответов.
- **Qdrant**: Векторная база данных для хранения эмбеддингов.
- **PostgreSQL**: Реляционная база данных для хранения данных приложений (n8n, Flowise, Langfuse).
- **Redis**: Кэширование данных.
- **Ollama**: Локальный сервер для запуска LLM (опционально).
- **Langfuse**: Платформа для мониторинга и отладки LLM-приложений.
- **Grafana**: Визуализация метрик и мониторинг системы.

## Предварительные требования

- Docker Engine (версия 24.0 или выше)
- Docker Compose (версия 2.20 или выше)
- Git (для клонирования репозитория)
- Свободное место на диске: минимум 20 ГБ (рекомендуется 50 ГБ+)
- ОЗУ: минимум 16 ГБ (рекомендуется 32 ГБ, особенно при использовании локальных моделей)

## Инструкция по развертыванию

### 1. Подготовка конфигурации

В корневой директории проекта находится файл `.env`. Этот файл содержит все основные настройки системы.

**Настройка LLM (Языковой модели) и Embedding (Векторизации):**

По умолчанию система настроена на использование DeepSeek для генерации и Qwen для векторизации через OpenRouter.

- **Вариант 1: Использование OpenRouter (по умолчанию)**
  Убедитесь, что в `.env` раскомментированы и заполнены следующие строки:

  ```env
  # LLM
  LLM_BASE_URL=https://openrouter.ai/api/v1
  LLM_API_KEY=ваш_ключ_api
  LLM_MODEL=deepseek/deepseek-r1-0528:free

  # Embedding
  EMBEDDING_MODEL=qwen/qwen3-embedding-8b
  EMBEDDING_BASE_URL=https://openrouter.ai/api/v1
  EMBEDDING_API_KEY=ваш_ключ_api
  ```

- **Вариант 2: Использование локальной Ollama**
  Для переключения на локальную модель выполните следующие действия:
  1.  Откройте файл `.env`.
  2.  Закомментируйте строки раздела "Вариант 1: OpenRouter".
  3.  Раскомментируйте строки раздела "Вариант 2: Локальная Ollama":

      ```env
      # LLM
      LLM_BASE_URL=http://ollama:11434/v1
      LLM_API_KEY=ollama
      LLM_MODEL=llama3

      # Embedding
      EMBEDDING_MODEL=nomic-embed-text
      EMBEDDING_BASE_URL=http://ollama:11434/v1
      EMBEDDING_API_KEY=ollama
      ```

  4.  После запуска контейнеров необходимо загрузить модели в Ollama (см. раздел "Запуск").

**Безопасность:**

Рекомендуется сменить стандартные пароли в файле `.env` перед запуском в продуктивной среде:

- `POSTGRES_PASSWORD`
- `QDRANT_API_KEY`
- `N8N_BASIC_AUTH_PASSWORD`
- `FLOWISE_PASSWORD`
- `GF_SECURITY_ADMIN_PASSWORD`
- `LANGFUSE_INIT_USER_PASSWORD`

### 2. Запуск системы

Выполните следующую команду в корневой директории проекта для сборки и запуска всех контейнеров:

```bash
docker-compose up -d
```

Эта команда скачает необходимые образы, соберет кастомные сервисы (Docling, LightRAG) и запустит их в фоновом режиме.

### 3. Проверка статуса

Убедитесь, что все контейнеры запущены и работают корректно:

```bash
docker-compose ps
```

Все сервисы должны иметь статус `Up`.

Если вы используете локальную Ollama, загрузите необходимую модель:

```bash
docker exec -it ollama ollama pull llama3
```

(Замените `llama3` на название нужной вам модели).

## Доступ к сервисам

После успешного запуска сервисы будут доступны по следующим адресам:

| Сервис           | URL                          | Логин по умолчанию             | Пароль по умолчанию |
| :--------------- | :--------------------------- | :----------------------------- | :------------------ |
| **Open WebUI**   | `http://localhost:3003`      | (регистрация при первом входе) | -                   |
| **n8n**          | `http://localhost:5678`      | `admin`                        | `secure_password`   |
| **Flowise**      | `http://localhost:3002`      | `admin`                        | `secure_password`   |
| **Langfuse**     | `http://localhost:3000`      | `admin@example.com`            | `secure_password`   |
| **Grafana**      | `http://localhost:3001`      | `admin`                        | `secure_password`   |
| **Docling API**  | `http://localhost:8000/docs` | -                              | -                   |
| **LightRAG API** | `http://localhost:8001/docs` | -                              | -                   |

## Использование

### Загрузка документов и общение

1.  **Через Open WebUI**:
    - Откройте `http://localhost:3003`.
    - Зарегистрируйте первого пользователя (он станет администратором).
    - В настройках подключения убедитесь, что URL и ключ API соответствуют настройкам в `.env` (Open WebUI автоматически подхватывает переменные окружения при старте, но их можно переопределить в интерфейсе).
    - Для работы с документами используйте встроенные возможности RAG в Open WebUI или настройте интеграцию с LightRAG через API.

2.  **Через n8n (Автоматизация)**:
    - Откройте `http://localhost:5678`.
    - Создайте новый workflow.
    - Вы можете использовать HTTP Request ноды для отправки документов в сервис Docling (`http://docling:8000/convert`) и последующей отправки текста в LightRAG (`http://lightrag:8000/ingest`).

3.  **Через Flowise**:
    - Откройте `http://localhost:3002`.
    - Создайте новый чат-флоу.
    - Используйте компоненты для подключения к Qdrant и вашей LLM.

### Мониторинг

- **Langfuse**: Используется для отслеживания качества ответов LLM, затрат токенов и трассировки запросов. Проект уже настроен, ключи доступа находятся в `.env`.
- **Grafana**: Используется для визуализации системных метрик (CPU, RAM контейнеров) и метрик приложений, если настроен экспорт.

## Устранение неполадок

**Контейнер не запускается:**
Проверьте логи конкретного контейнера:

```bash
docker-compose logs -f <имя_сервиса>
```

Например: `docker-compose logs -f lightrag`

**Проблемы с подключением к БД:**
Убедитесь, что контейнер `postgres` перешел в статус `healthy` перед тем, как зависимые сервисы начали работу. Docker Compose файл настроен на ожидание, но при первом запуске инициализация базы может занять время.

**Ошибки LLM:**

- Если используется OpenRouter: проверьте баланс и правильность API ключа.
- Если используется Ollama: убедитесь, что модель загружена (`ollama list`) и сервису хватает оперативной памяти.

## Остановка системы

Для остановки всех сервисов выполните:

```bash
docker-compose down
```

Чтобы остановить сервисы и удалить тома с данными (ВНИМАНИЕ: все данные будут потеряны):

```bash
docker-compose down -v
```
