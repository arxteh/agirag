# Руководство по развертыванию платформы Agi RAG

Этот документ содержит инструкции по развертыванию и использованию on-premise платформы для взаимодействия с документами и демонстрации возможностей ИИ. Платформа объединяет инструменты для обработки документов, векторного поиска, автоматизации рабочих процессов и пользовательские интерфейсы.

## Архитектура

Система состоит из следующих компонентов:

- **Open WebUI**: Пользовательский интерфейс для общения в чате.
- **n8n**: Инструмент автоматизации рабочих процессов.
- **Flowise**: Визуальный конструктор для LLM-приложений.
- **Docling**: Сервис для конвертации документов в машиночитаемый формат (Markdown).
- **LightRAG**: Система RAG (Retrieval-Augmented Generation) для поиска и генерации ответов.
- **Qdrant**: Векторная база данных для хранения эмбеддингов.
- **PostgreSQL**: Реляционная база данных для данных приложений (n8n, Flowise, Langfuse).
- **Redis**: Кэширование данных.
- **Ollama**: Локальный сервер для запуска LLM (опционально).
- **Langfuse**: Платформа для мониторинга и отладки LLM-приложений.
- **Grafana**: Визуализация системных метрик и мониторинг.
- **MLflow**: Реестр экспериментов и моделей (MLOps).
- **Prometheus + cAdvisor**: Метрики контейнеров (опционально).

## Предварительные требования

- Docker Engine (версия 24.0 или выше)
- Docker Compose (версия 2.20 или выше)
- Git (для клонирования репозитория)
- Дисковое пространство: Минимум 20 ГБ (Рекомендуется 50 ГБ+)
- ОЗУ: Минимум 16 ГБ (Рекомендуется 32 ГБ, особенно при использовании локальных моделей)

## Конфигурация

Файл `.env` в корневой директории содержит все основные настройки системы.

### Конфигурация LLM и Embedding

Стек разработан для работы с **OpenAI-compatible API**.

Конфигурация по умолчанию использует OpenRouter:

- **LLM**: `deepseek/deepseek-r1-0528:free`
- **Embedding**: `qwen/qwen3-embedding-8b`

#### 1. OpenRouter (По умолчанию)

Убедитесь, что в `.env` установлены следующие переменные:

```env
LLM_MODEL=deepseek/deepseek-r1-0528:free
LLM_BASE_URL=https://openrouter.ai/api/v1
LLM_API_KEY=

EMBEDDING_MODEL=qwen/qwen3-embedding-8b
EMBEDDING_BASE_URL=https://openrouter.ai/api/v1
EMBEDDING_API_KEY=
EMBEDDING_DIM=1024
```

#### 2. Переключение на локальную Ollama

Для использования локального экземпляра Ollama:

1.  Откройте `.env`.
2.  Измените `LLM_BASE_URL` и `EMBEDDING_BASE_URL` на `http://ollama:11434/v1`.
3.  Установите `LLM_API_KEY` и `EMBEDDING_API_KEY` в значение `ollama`.
4.  Обновите названия моделей на те, что доступны в вашем экземпляре Ollama.

Пример:

```env
LLM_MODEL=llama3
LLM_BASE_URL=http://ollama:11434/v1
LLM_API_KEY=ollama

EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_BASE_URL=http://ollama:11434/v1
EMBEDDING_API_KEY=ollama
EMBEDDING_DIM=768
```

**Примечание по эмбеддингам:**
Векторизация используется в RAG (LightRAG, Flowise/Open WebUI). Критически важно правильно указать `EMBEDDING_DIM` (размерность эмбеддинга) для выбранной модели, иначе индексация может не сработать.

### Внутренние адреса сервисов

Для стабильной работы рабочих процессов n8n внутренние адреса сервисов определены в системных переменных:

- `DOCLING_URL`: `http://docling:8000`
- `LIGHTRAG_URL`: `http://lightrag:8000`
- `QDRANT_URL`: `http://qdrant:6333`

## Развертывание

### Стандартный запуск (CPU)

```bash
docker compose up -d
```

### Запуск с поддержкой GPU (Локальный GPU)

Для включения поддержки GPU для локальной Ollama (требуется NVIDIA Container Toolkit):

```bash
docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d
```

Эта опция добавляет резервирование ресурсов GPU для контейнера Ollama, не нарушая конфигурацию для машин без видеокарты (если файл переопределения не используется).

### Профиль наблюдаемости (Observability)

Для включения Prometheus и cAdvisor:

```bash
docker compose --profile observability up -d
```

## Статус сервисов и доступ

После запуска сервисы доступны по следующим адресам:

| Сервис           | URL                               |
| :--------------- | :-------------------------------- |
| **Open WebUI**   | `http://localhost:3003`           |
| **n8n**          | `http://localhost:5678`           |
| **Flowise**      | `http://localhost:3002`           |
| **Langfuse**     | `http://localhost:3000`           |
| **Grafana**      | `http://localhost:3001`           |
| **Prometheus**   | `http://localhost:9090`           |
| **cAdvisor**     | `http://localhost:8081`           |
| **MLflow**       | `http://localhost:5000`           |
| **Docling API**  | `http://localhost:8000/docs`      |
| **LightRAG API** | `http://localhost:8001/docs`      |
| **Qdrant**       | `http://localhost:6333/dashboard` |

**Примечание:** Сервисы, такие как n8n и Flowise, настроены на ожидание готовности базы данных PostgreSQL (Healthchecks) перед запуском.

## Использование

### 1. Open WebUI

- Откройте `http://localhost:3003`.
- Зарегистрируйте первого пользователя (администратор).
- Настройте подключения при необходимости, хотя переменные окружения должны предварительно настроить соединение.

### 2. n8n (Автоматизация)

- Откройте `http://localhost:5678`.
- Используйте предварительно настроенные переменные окружения (`DOCLING_URL`, `LIGHTRAG_URL`, `QDRANT_URL`) в ваших рабочих процессах.

### 3. Flowise

- Откройте `http://localhost:3002`.
- Создавайте чат-флоу, используя доступные компоненты.

### 4. Мониторинг

- **Langfuse**: Для трассировки запросов LLM.
- **Grafana**: Для системных метрик.

## Устранение неполадок

- **Контейнер не запускается**: Проверьте логи с помощью `docker compose logs -f <имя_сервиса>`.
- **Подключение к базе данных**: Убедитесь, что контейнер `postgres` находится в состоянии `healthy`.
- **Ошибки LLM**: Проверьте API ключи и названия моделей в `.env`. При использовании Ollama убедитесь, что модель загружена (`docker exec -it ollama ollama pull <имя_модели>`).

## Остановка системы

Остановить все сервисы:

```bash
docker compose down
```

Остановить и удалить тома (ВНИМАНИЕ: Потеря данных):

```bash
docker compose down -v
```
